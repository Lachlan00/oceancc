# drop failed tracks and names
track_ls <- track_ls[!fail_bool]
name_ls <- name_ls[!fail_bool]
print(paste(length(fail_ls),'tracks failed to load..'))
# process names column into trackid values
id_ls <- name_ls
str_split(track_ls, '-')
id_ls
id_ls
x <- id_ls[1]
str_split(x, '-')[2]
str_split(x, '-')
str_split(x, '-')[2]
str_split(x, '-')[1][2]
str_split(x, '-')[[1]][2]
id_ls <- lapply(id_ls, function(x) str_split(x, '-')[[1]][2])
id_ls
# process names column into trackid values
id_ls <- name_ls
id_ls <- unlist(lapply(id_ls, function(x) str_split(x, '-')[[1]][2]))
id_ls
# process names column into trackid values
id_ls <- name_ls
id_ls <- sapply(id_ls, function(x) str_split(x, '-')[[1]][2])
id_ls
# remove unwanted strings
id_ls <- lapply(id_ls, function(x) sub("(accel)", "", x))
id_ls
# process names column into trackid values
id_ls <- name_ls
id_ls <- sapply(id_ls, function(x) str_split(x, '-')[[1]][2])
# remove unwanted strings
id_ls <- sapply(id_ls, function(x) sub("\(accel\)", "", x))
# process names column into trackid values
id_ls <- name_ls
id_ls <- sapply(id_ls, function(x) str_split(x, '-')[[1]][2])
# remove unwanted strings
id_ls <- sapply(id_ls, function(x) sub("\\(accel\\)", "", x))
id_ls
# remove unwanted strings
id_ls <- unlist(sapply(id_ls, function(x) sub("\\(accel\\)", "", x)))
id_ls
# process names column into trackid values
id_ls <- name_ls
id_ls <- unlist(lapply(id_ls, function(x) str_split(x, '-')[[1]][2]))
# remove unwanted strings
id_ls <- unlist(lapply(id_ls, function(x) sub("\\(accel\\)", "", x)))
id_ls
# split on space
id_ls <- unlist(lapply(id_ls, function(x) str_split(x, ' ')[[1]][2]))
id_ls
# process names column into trackid values
id_ls <- name_ls
id_ls <- unlist(lapply(id_ls, function(x) str_split(x, '-')[[1]][2]))
# remove unwanted strings
id_ls <- unlist(lapply(id_ls, function(x) sub("\\(accel\\)", "", x)))
# split on space
id_ls <- unlist(lapply(id_ls, function(x) str_split(x, ' ')[[1]][1]))
id_ls
# process names column into trackid values
id_ls <- name_ls
id_ls <- unlist(lapply(id_ls, function(x) str_split(x, '-')[[1]][2]))
# remove unwanted strings
id_ls <- unlist(lapply(id_ls, function(x) sub("\\(accel\\)", "", x)))
# fix isolated case
id_ls <- unlist(lapply(id_ls, function(x)sub("095314 LP 0526", "0526", x)))
# split on space
id_ls <- unlist(lapply(id_ls, function(x) str_split(x, ' ')[[1]][1]))
id_ls
# create list of datetimes of first fixes (for id ordering)
date_ls <- lapply(track_ls, function(x) x$Date[1])
date_ls
# create list of datetimes of first fixes (for id ordering)
date_ls <- unlist(lapply(track_ls, function(x) x$Date[1]))
date_ls
# create list of datetimes of first fixes (for id ordering)
date_ls <- unlist(lapply(track_ls, function(x) as.character(x$Date[1])))
date_ls
time_ls <- unlist(lapply(track_ls, function(x) as.character(x$Time[1])))
time_ls
# remove white space
time_ls <- unlist(lapply(time_ls, function(x)sub(" ", "", x)))
time_ls
# make datetime (time is local)
dt_ls <- list()
for (i in 1:length(id_ls)){
dt <- as.POSIXct(paste(date_ls[i], time_ls[i]), tz="Australia/Sydney")
dt_ls[[i]] <- dt
}
dt_ls <- unlist(dt_ls)
dt_ls
?which(dt_ls < 0)
which(dt_ls < 0)
name)ls[82]
name_ls[82]
date_ls[82]
date_ls[83]
time_ls[83]
track_ls[83]
id_ls[82]
id_ls[83]
name_ls[83]
# some datetimes are not formatted correctly so use this opportunity
# to reverse the date on the messed up one
bad_idx <- which(dt_ls < 0)
bad_idx
# some datetimes are not formatted correctly so use this opportunity
# to reverse the date on the messed up one
bad_idx <- c(which(dt_ls < 0))
bad_idx
for (i in bad_idx){print(i)}
# some datetimes are not formatted correctly so use this opportunity
# to reverse the date on the messed up one
badDate_idx <- c(which(dt_ls < 0))
# fix the date stamps
for (i in badDate_idx){
track_ls[[i]]$Date <- strptime(as.character(track_ls[[i]]$Date$date), "%d/%m/%Y")
}
# fix the date stamps
for (i in badDate_idx){
track_ls[[i]]$Date <- strptime(as.character(track_ls[[i]]$Date), "%d/%m/%Y")
}
head(track_ls[[83]])
# cause of that we now have to repeat it all again!
# create list of datetimes of first fixes (for id ordering)
date_ls <- unlist(lapply(track_ls, function(x) as.character(x$Date[1])))
time_ls <- unlist(lapply(track_ls, function(x) as.character(x$Time[1])))
# remove white space
time_ls <- unlist(lapply(time_ls, function(x)sub(" ", "", x)))
# make datetime (time is local)
dt_ls <- list()
for (i in 1:length(id_ls)){
dt <- as.POSIXct(paste(date_ls[i], time_ls[i]), tz="Australia/Sydney")
dt_ls[[i]] <- dt
}
dt_ls <- unlist(dt_ls)
dt_ls
id_ls
as.factor(id_ls)
levels(as.factor(id_ls))
# report tracks and individuals
print(paste('There are',length(id_ls),'tracks for',length(levels(as.factor(id_ls))),' individual penguins.'))
# report tracks and individuals
print(paste('There are',length(id_ls),'tracks for',length(levels(as.factor(id_ls))),'individual penguins.'))
# report tracks and individuals
print(paste('There are',length(id_ls),'tracks from',length(levels(as.factor(id_ls))),'individuals'))
sumarry(as.factor(id_ls))
summary(as.factor(id_ls))
# load in data
df1 <- readRDS('data/loaded_KMLtracks.rds') # preloaded kml files
id_counts <- levels(as.factor(id_ls))
id_counts['0511M']
id_counts <- summary(as.factor(id_ls))
id_counts['0511M']
# get the ordered index by timestamps
order_idx <- order(dt_ls)
order_idx
# generate id names based on number of individuals
new_names <- 1:length(levels(as.factor(id_ls)))
new_names
library(stringr)
new_names <- str_pad(new_names, 2, pad = "0")
new_names
id_ls
id_ls[order_idx]
# generate id names based on number of individuals
new_name_set <- 1:length(levels(as.factor(id_ls)))
new_name_set <- str_pad(new_names, 2, pad = "0")
# make the new ordered lists (by first timestamp)
tracks_ls <- tracks_ls[order_idx]
id_ls <- id_ls[order_idx]
# make the new ordered lists (by first timestamp)
track_ls <- tracks_ls[order_idx]
# make the new ordered lists (by first timestamp)
track_ls <- track_ls[order_idx]
# now we iterate through the dataset and apply the new names
new_names <- list()
# now we iterate through the dataset and apply the new names
new_names <- list()
name_already_given <- list()
i <- 1
id_ls[i] %in% name_already_given
!(id_ls[i] %in% name_already_given
)
setwd("~/Development/PhD/repos/penguin_tracks")
library(mpm)
source("data_scripts/data_functions.R")
df1 <- readRDS('data/loaded_KMLtracks.rds') # preloaded kml files
# clean data files
df2 <- ocean_points_all(df1) # isolate foraging trips
df3 <- split_trips(df2) # split multi trips
df4 <- speed_clean(df3, maxSpeed=10) # in m/s
# note other speed filers such `argosfilter` used by `foieGras`.
# make 2m predicted paths
df5 = crawl_predict_all(df4, 5, view_map=FALSE)
df <- df5
# Like in the last example we need to tell R that out dtUTC column is
# a collection of date time objects.
# I want to display the datetimes as local time at the end so I have
# made local datetime column. I always like to keep the UTC column too
# to make sure everything is behaving right.
df$dtAEST <- df$dtUTC
attr(df$dtAEST, "tzone") <- "Australia/Sydney"
mapfile <- readRDS("./data/mapfile.rds")
mapfile <- readRDS("./assets/maps/hmm_mapfile.rds")
df$year <- as.factor(year(df$dtAEST))
peng.map <- mapfile +
scale_y_continuous(limits = c(min(df$lat, na.rm=TRUE)-0.01, max(df$lat, na.rm=TRUE)+0.01), expand = c(0, 0)) +
scale_x_continuous(limits = c(min(df$lon, na.rm=TRUE)-0.07, max(df$lon, na.rm=TRUE)+0.07), expand = c(0, 0)) +
geom_point(data=df, aes(x=lon,y=lat,color=year), size=1.5)
df$hour <- hour(df$dtAEST)
df$min <- minute(df$dtAEST)
df$datetime_relative <- ISOdatetime(2000, 01, 01, as.integer(df$hour), as.integer(df$min), 0)
peng.anim <- peng.map +
transition_time(df$datetime_relative) +
shadow_wake(wake_length = 0.2, alpha = TRUE)
# finally we will save the animation as a gif file so we can take it home
# to show mum and dad.
anim_save("peng_anim_year.gif", peng.anim)
# Process Gemma's tracks
input_dir_ls = c('/Volumes/LP_MstrData/master-data/gemma/Montague\ Island/Data/Montague\ Island\ penguin\ track\ data\ 2012/',
'/Volumes/LP_MstrData/master-data/gemma/Montague\ Island/Data/Montague\ Island\ penguin\ track\ data\ 2013/',
'/Volumes/LP_MstrData/master-data/gemma/Montague\ Island/Data/Montague\ Island\ penguin\ track\ data\ 2014/')
# Process Gemma's tracks
input_dir_ls = c('/Volumes/LP_MstrData/master-data/gemma/Montague\ Island/Data/Montague\ Island\ penguin\ track\ data\ 2012/',
'/Volumes/LP_MstrData/master-data/gemma/Montague\ Island/Data/Montague\ Island\ penguin\ track\ data\ 2013/',
'/Volumes/LP_MstrData/master-data/gemma/Montague\ Island/Data/Montague\ Island\ penguin\ track\ data\ 2014/')
# get all the tracks files paths
path_ls <- list()
name_ls <- list()
i <- 1
for (dir in input_dir_ls){
file_ls <- list.files(path=dir, pattern=".csv$", recursive=FALSE)
name_ls[[i]] <- file_ls
file_ls <- lapply(file_ls, function(x) paste0(dir, x))
path_ls[[i]] <- file_ls
i <- i + 1
}
path_ls <- unlist(path_ls)
name_ls <- unlist(name_ls)
# remove ".csv" in names
name_ls <- gsub(".csv", "", name_ls)
# drop "uplink times.csv"
path_ls <- path_ls[!grepl("Uplink times.csv" , path_ls)]
name_ls <- name_ls[!grepl("Uplink times" , name_ls)]
# read in all of the data
print(paste('Reading in', length(name_ls), 'legacy tracks..'))
track_ls <- list()
fail_ls <- list()
i <- 1
j <- 1
for (path in path_ls){
# check header for corrupt files
header <- readLines(file(path, 'r'), n=1)
corrupt <- grepl("MONTAGUE", header) | grepl("TIMEVALUE", header)
# if file not corrupt try to read
if (!corrupt){
print(paste('Loading track', i, 'of', length(path_ls)))
track_ls[[i]] <- read.csv(path)
} else {
fail_ls[[j]] <- name_ls[i]
j <- j + 1
}
i <- i + 1
}
print(paste(length(fail_ls),'tracks failed to load..'))
# remove empty failed loads
fail_bool <- unlist(lapply(track_ls, is.null))
# drop failed tracks and names
track_ls <- track_ls[!fail_bool]
name_ls <- name_ls[!fail_bool]
# process names column into trackid values
id_ls <- name_ls
id_ls <- unlist(lapply(id_ls, function(x) str_split(x, '-')[[1]][2]))
# remove unwanted strings
id_ls <- unlist(lapply(id_ls, function(x) sub("\\(accel\\)", "", x)))
# fix isolated case
id_ls <- unlist(lapply(id_ls, function(x)sub("095314 LP 0526", "0526", x)))
# split on space
id_ls <- unlist(lapply(id_ls, function(x) str_split(x, ' ')[[1]][1]))
# create list of datetimes of first fixes (for id ordering)
date_ls <- unlist(lapply(track_ls, function(x) as.character(x$Date[1])))
time_ls <- unlist(lapply(track_ls, function(x) as.character(x$Time[1])))
# remove white space
time_ls <- unlist(lapply(time_ls, function(x)sub(" ", "", x)))
# make datetime (time is local)
dt_ls <- list()
for (i in 1:length(id_ls)){
dt <- as.POSIXct(paste(date_ls[i], time_ls[i]), tz="Australia/Sydney")
dt_ls[[i]] <- dt
}
dt_ls <- unlist(dt_ls)
# some datetimes are not formatted correctly so use this opportunity
# to reverse the date on the messed up one
badDate_idx <- c(which(dt_ls < 0))
# fix the date stamps
for (i in badDate_idx){
track_ls[[i]]$Date <- strptime(as.character(track_ls[[i]]$Date), "%d/%m/%Y")
}
# cause of that we now have to repeat it all again!
# create list of datetimes of first fixes (for id ordering)
date_ls <- unlist(lapply(track_ls, function(x) as.character(x$Date[1])))
time_ls <- unlist(lapply(track_ls, function(x) as.character(x$Time[1])))
# remove white space
time_ls <- unlist(lapply(time_ls, function(x)sub(" ", "", x)))
# make datetime (time is local)
dt_ls <- list()
for (i in 1:length(id_ls)){
dt <- as.POSIXct(paste(date_ls[i], time_ls[i]), tz="Australia/Sydney")
dt_ls[[i]] <- dt
}
dt_ls <- unlist(dt_ls)
# report tracks and individuals
print(paste('There are',length(id_ls),'tracks from',length(levels(as.factor(id_ls))),'individuals'))
# get look up table for ids for multiple tracks
id_counts <- summary(as.factor(id_ls))
# get the ordered index by timestamps
order_idx <- order(dt_ls)
# generate id names based on number of individuals
new_name_set <- 1:length(levels(as.factor(id_ls)))
new_name_set <- str_pad(new_names, 2, pad = "0")
# make the new ordered lists (by first timestamp)
track_ls <- track_ls[order_idx]
id_ls <- id_ls[order_idx]
# now we iterate through the dataset and apply the new names
new_names <- list()
name_already_given <- list()
# Rstats for Oceancc Paper
# Basic statistical tests for paper
# 1. check chnage over time
# 2. check change in dominance bewteen first and last 5 years
library(ggplot2)
library(lubridate)
library(lsmeans)
library(dplyr)
# Setup
setwd("~/Development/PhD/repos/oceancc")
# get data
df_Jerv <- read.csv('./data/count_Jerv.csv')
df_Jerv$site <- 'Jerv'
df_Bate <- read.csv('./data/count_Bate.csv')
df_Bate$site <- 'Bate'
df_Howe <- read.csv('./data/count_Howe.csv')
df_Howe$site <- 'Howe'
# bind data
df <- do.call('rbind', list(df_Jerv, df_Bate, df_Howe))
# convert string to date
df$dt <- as.POSIXct(df$dt)
# convert string to factor
df$site <- as.factor(df$site)
# reorder factors (so in order Jerv, Bate, Howe)
df$site = factor(df$site,levels(df$site)[c(3,1,2)])
# make Ratio data
df$ratioA = df$countA/(df$countA + df$countB)
df$ratioB = df$countB/(df$countA + df$countB)
# add year and month
df$year <- year(df$dt)
df$month <- month(df$dt)
df$day <- day(df$dt)
################################
# get monthly and yearly means #
################################
i <- 1
df_ls <- split(df, df$site)
df_year <- list()
df_month <- list()
for (dat in df_ls){
df_year[[i]] <- aggregate(dat, by=list(dat$year), mean)
df_month[[i]] <- aggregate(dat, by=list(dat$year, dat$month), mean)
df_year[[i]]$site <- dat$site[1]
df_month[[i]]$site <- dat$site[1]
i <- i +1
}
df_year <- do.call('rbind', df_year)
df_month <- do.call('rbind',df_month)
# drop 2016 in years
df_year <- df_year[!df_year$year == 2016,]
#############################################
# extract seasonal components for each site #
#############################################
# drop 2016
df_season <- df[df$year != 2016,]
df_season_ls <- split(df_season, df_season$site)
# get yearly means
for (i in 1:length(df_season_ls)){
dat <- df_season_ls[[i]]
site_str <- dat$site
df_season_ls[[i]] <- aggregate(dat, by=list(dat$day, dat$month), mean)
df_season_ls[[i]]$site <- as.character(site_str[1])
df_season_ls[[i]]$DoY <- 1:length(df_season_ls[[i]]$site)
}
# merge
df_season_all <- do.call('rbind', df_season_ls)
# extract first and last 5 years
df_first <- df_season[df_season$year < 1999, ]
df_last <- df_season[df_season$year > 2010, ]
# get seasonal means for first and last half
# first
df_first_ls <- split(df_first, df_first$site)
# get yearly means
for (i in 1:length(df_first_ls)){
dat <- df_first_ls[[i]]
site_str <- dat$site
df_first_ls[[i]] <- aggregate(dat, by=list(dat$day, dat$month), mean)
df_first_ls[[i]]$site <- as.character(site_str[1])
df_first_ls[[i]]$DoY <- 1:length(df_first_ls[[i]]$site)
}
# merge
df_season_first <- do.call('rbind', df_first_ls)
# fix factors
df_season_first$site <- as.factor(df_season_first$site)
df_season_first$site = factor(df_season_first$site,levels(df_season_first$site)[c(3,1,2)])
# last
df_last_ls <- split(df_last, df_last$site)
# get yearly means
for (i in 1:length(df_last_ls)){
dat <- df_last_ls[[i]]
site_str <- dat$site
df_last_ls[[i]] <- aggregate(dat, by=list(dat$day, dat$month), mean)
df_last_ls[[i]]$site <- as.character(site_str[1])
df_last_ls[[i]]$DoY <- 1:length(df_last_ls[[i]]$site)
}
# merge
df_season_last <- do.call('rbind', df_last_ls)
# fix factors
df_season_last$site <- as.factor(df_season_last$site)
df_season_last$site = factor(df_season_last$site,levels(df_season_last$site)[c(3,1,2)])
#####################################
# Raw ratio data first last 5 years #
#####################################
# calculate proportion of yearly dominance
df$dombool <- df$ratioA > 0.5
df_dom_site_ls <- split(df, df$site)
j <- 1
for (df in df_dom_site_ls){
df_dom_ls <- split(df, df$year)
i <- 1
for (df in df_dom_ls){
dom_count <- sum(df$dombool)
df_dom_ls[[i]] <- dom_count
i <- i + 1
}
df_dom <- do.call('rbind', df_dom_ls)
df_dom <- data.frame(year = as.numeric(rownames(df_dom)),
dom = df_dom[1:length(df_dom)])
df_first <- df_dom[df_dom$year < 1999, ]
df_first$test <- 'first'
df_last <- df_dom[df_dom$year > 2010, ]
df_last <- df_last[df_last$year != 2016, ]
df_last$test <- 'last'
# merge
df_dom <- rbind(df_first, df_last)
df_dom_site_ls[[j]] <- df_dom
j <- j + 1
}
####################################
# Change in long term EAC dominace #
####################################
# plot loess
ggplot(df_year, aes(x=year, y=ratioA, colour=site)) +
geom_point(alpha=0.4) +
stat_smooth(data=subset(df_year, site == "Jerv"), method = "gam", formula = y ~ s(x)) +
stat_smooth(data=subset(df_year, site == "Bate"), method = "gam", formula = y ~ s(x)) +
stat_smooth(data=subset(df_year, site == "Howe"), method = "gam", formula = y ~ s(x)) +
ggtitle('Linear Regression of mean regional EAC dominance')
# run the analysis
print('\nModel all year means')
fit <- lm(formula = ratioA ~ year*site, data = df_year)
anova(fit)
# use logisitc regression
df_logistic <- df_year
df_logistic$countA_int <- round(df_logistic$countA)
df_logistic$countB_int <- round(df_logistic$countB)
fit_logis <- glm(cbind(countA_int, countA_int + countB_int) ~ year, df_logistic, subset=site=="Jerv", family = binomial)
summary(fit_logis)
fit_logis <- glm(cbind(countA_int, countA_int + countB_int) ~ year, df_logistic, subset=site=="Bate", family = binomial)
summary(fit_logis)
fit_logis <- glm(cbind(countA_int, countA_int + countB_int) ~ year, df_logistic, subset=site=="Howe", family = binomial)
summary(fit_logis)
# Test to confirm rate of change same between sites?
###########################################################
# Check seasonal change before the first and last 5 years #
###########################################################
# check the data with plots
ggplot(df_season_all, aes(x=DoY, y=ratioA, colour=site)) +
geom_line() + ggtitle('All sites means seasonality')
# Check first vs last half
ggplot(df_season_first, aes(x=DoY, y=ratioA)) +
geom_line(data = df_season_first, colour='blue') +
geom_line(data = df_season_last, colour='red') +
facet_wrap(~site, dir='v')
# test the change between first and last 5 years between each site
# jerv
fit_change <- glm(dom ~ test, df_dom_site_ls[['Jerv']], family = poisson)
summary(fit_change)
# bate
fit_change <- glm(dom ~ test, df_dom_site_ls[['Bate']], family = poisson)
summary(fit_change)
# howe
fit_change <- glm(dom ~ test, df_dom_site_ls[['Howe']], family = poisson)
summary(fit_change)
df_dom_site_merge <- do.call(rbind, df_dom_site_ls) %>%
mutate(site = str_split(row.names(.), "\\.", simplify = TRUE)[,1])
fit_change_all <- glm(dom ~ test*site, df_dom_site_merge, family = poisson)
summary(fit_change_all)
# plot loess
ggplot(df_year, aes(x=year, y=ratioA, colour=site)) +
geom_point(alpha=0.4) +
stat_smooth(data=subset(df_year, site == "Jerv"), method = "gam", formula = y ~ s(x)) +
stat_smooth(data=subset(df_year, site == "Bate"), method = "gam", formula = y ~ s(x)) +
stat_smooth(data=subset(df_year, site == "Howe"), method = "gam", formula = y ~ s(x)) +
ggtitle('GAM of yearly EAC dominance period')
